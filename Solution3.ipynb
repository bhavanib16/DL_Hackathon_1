{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import pandas as pd\n",
        "import os\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "R9ActCouQEOf"
      },
      "id": "R9ActCouQEOf",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzipping the dataset\n",
        "with zipfile.ZipFile('/content/train_SOaYf6m.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data')"
      ],
      "metadata": {
        "id": "FGnSOuYgk4Aa"
      },
      "id": "FGnSOuYgk4Aa",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "187b841b-b983-447a-9c5d-2c4f5fd76939",
      "metadata": {
        "id": "187b841b-b983-447a-9c5d-2c4f5fd76939"
      },
      "outputs": [],
      "source": [
        "# Loading the training data and testing data\n",
        "train_df = pd.read_csv('/content/data/train.csv')\n",
        "test_df = pd.read_csv('/content/data/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the training data into training and validation sets\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "21QVS5hqQdX0"
      },
      "id": "21QVS5hqQdX0",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining image path\n",
        "image_path = '/content/data/images'"
      ],
      "metadata": {
        "id": "SaiwJhK9QiAT"
      },
      "id": "SaiwJhK9QiAT",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset class for loading images\n",
        "class VehicleDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "        label = self.dataframe.iloc[idx, 1] if 'emergency_or_not' in self.dataframe.columns else -1\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "WVGI6iCkQni3"
      },
      "id": "WVGI6iCkQni3",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining transformations for the training data with data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Defining transformations for the testing data\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "mBarxC3lQoUo"
      },
      "id": "mBarxC3lQoUo",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b4d7a2-21df-418e-8f63-b0a222c13cc8",
      "metadata": {
        "id": "f1b4d7a2-21df-418e-8f63-b0a222c13cc8"
      },
      "outputs": [],
      "source": [
        "# Define transformations for the images\n",
        "#transform = transforms.Compose([\n",
        "#    transforms.Resize((128, 128)),\n",
        "#    transforms.ToTensor(),\n",
        "#])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating datasets and dataloaders\n",
        "train_dataset = VehicleDataset(train_df, image_path, transform=train_transform)\n",
        "val_dataset = VehicleDataset(val_df, image_path, transform=train_transform)\n",
        "test_dataset = VehicleDataset(test_df, image_path, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "yvWCcpHAGLqH"
      },
      "id": "yvWCcpHAGLqH",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8a1fb92d-b4eb-41fa-836c-03361156e7c7",
      "metadata": {
        "id": "8a1fb92d-b4eb-41fa-836c-03361156e7c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c60fd4c5-48d9-4cf5-b513-ce3b84e3f7dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 172MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Loading pre-trained ResNet18 model and modify the final layer\n",
        "model = models.resnet18(pretrained=True)\n",
        "num_ftrs = model.fc.in_features\n",
        "# 2 classes: emergency and non-emergency\n",
        "model.fc = nn.Linear(num_ftrs, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a8680d3a-ec45-4a82-a6a4-193fee534d90",
      "metadata": {
        "id": "a8680d3a-ec45-4a82-a6a4-193fee534d90"
      },
      "outputs": [],
      "source": [
        "# Defining loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ed8f9a8a-c214-4acf-8951-eebec64637ec",
      "metadata": {
        "id": "ed8f9a8a-c214-4acf-8951-eebec64637ec",
        "outputId": "eeea8e37-9cb2-45bd-e888-034109e0e391",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.3668429055472925\n",
            "Validation Loss: 0.31993619013916363, Validation Accuracy: 0.8727272727272727\n",
            "Epoch 2/10, Loss: 0.2304074876010418\n",
            "Validation Loss: 1.061462489041415, Validation Accuracy: 0.806060606060606\n",
            "Epoch 3/10, Loss: 0.2788249823663916\n",
            "Validation Loss: 0.41898030042648315, Validation Accuracy: 0.8242424242424242\n",
            "Epoch 4/10, Loss: 0.1471666841812077\n",
            "Validation Loss: 0.29302086477929895, Validation Accuracy: 0.8727272727272727\n",
            "Epoch 5/10, Loss: 0.1050182053198417\n",
            "Validation Loss: 0.19386424191973425, Validation Accuracy: 0.9181818181818182\n",
            "Epoch 6/10, Loss: 0.09631109955011025\n",
            "Validation Loss: 0.33945635841651395, Validation Accuracy: 0.8848484848484849\n",
            "Epoch 7/10, Loss: 0.094530002435758\n",
            "Validation Loss: 0.33549097044901416, Validation Accuracy: 0.9303030303030303\n",
            "Epoch 8/10, Loss: 0.12634280723120486\n",
            "Validation Loss: 0.7297993898391724, Validation Accuracy: 0.8090909090909091\n",
            "Epoch 9/10, Loss: 0.26001822318704354\n",
            "Validation Loss: 0.5610571584918282, Validation Accuracy: 0.8151515151515152\n",
            "Epoch 10/10, Loss: 0.14248084059605995\n",
            "Validation Loss: 0.5299500118602406, Validation Accuracy: 0.796969696969697\n"
          ]
        }
      ],
      "source": [
        "# Training the model with validation\n",
        "num_epochs = 10\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "    # Validation step\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "    print(f\"Validation Loss: {val_loss/len(val_loader)}, Validation Accuracy: {val_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4e41cb2c-f724-490f-8e7f-e705ba7488ed",
      "metadata": {
        "id": "4e41cb2c-f724-490f-8e7f-e705ba7488ed"
      },
      "outputs": [],
      "source": [
        "# Predicting on the test data\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        predictions.extend(preds.cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e46742d9-2e90-4e9a-9ad0-8925811812c7",
      "metadata": {
        "id": "e46742d9-2e90-4e9a-9ad0-8925811812c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e99a3f3-f425-43fd-f34a-f40f1524b8ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification completed and results are saved to sample_submission.csv\n"
          ]
        }
      ],
      "source": [
        "# Creating the output predicted DataFrame\n",
        "submission_df = pd.DataFrame({'image_names': test_df['image_names'], 'emergency_or_not': predictions})\n",
        "\n",
        "# Saving the output to a CSV file\n",
        "submission_df.to_csv('sample_submission.csv', index=False)\n",
        "\n",
        "print(\"Classification completed and results are saved to sample_submission.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rz5E9vu-k2zS"
      },
      "id": "Rz5E9vu-k2zS",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}